{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLx5NdZ4SZuyUZq/WPp+Es",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jlakshmi/Pradhyapak_AI_Algomathics/blob/main/pradhyapak_ai_ver1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqcLYeROv7zv",
        "outputId": "4e8d6e7e-6aa8-4015-9b79-d63f082c0cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from bs4 import BeautifulSoup\n",
        "import gradio as gr\n",
        "\n",
        "# -------------------------------\n",
        "# Literature Recommendation (Semantic Scholar)\n",
        "# -------------------------------\n",
        "HISTORY_FILE = \"search_history.json\"\n",
        "\n",
        "# Load previous search history\n",
        "def load_history():\n",
        "    if os.path.exists(HISTORY_FILE):\n",
        "        with open(HISTORY_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return []\n",
        "\n",
        "# Save updated search history\n",
        "def save_history(history):\n",
        "    with open(HISTORY_FILE, \"w\") as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "# Search Semantic Scholar API (fixed endpoint)\n",
        "def search_papers(query, limit=5):\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"limit\": limit,\n",
        "        \"fields\": \"title,abstract,year,authors,url,openAccessPdf\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if \"data\" in data:\n",
        "                return data[\"data\"]\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        return []\n",
        "\n",
        "# Rank results based on history\n",
        "def rank_with_history(results, history):\n",
        "    ranked = []\n",
        "    for paper in results:\n",
        "        score = 0\n",
        "        for past_query in history:\n",
        "            if past_query and isinstance(past_query, str):\n",
        "                if past_query.lower() in str(paper.get(\"title\", \"\")).lower():\n",
        "                    score += 2\n",
        "                if past_query.lower() in str(paper.get(\"abstract\", \"\")).lower():\n",
        "                    score += 1\n",
        "        ranked.append((score, paper))\n",
        "    ranked.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [p for _, p in ranked]\n",
        "\n",
        "# Download paper PDF if available\n",
        "def download_paper(paper, folder=\"downloads\"):\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    pdf_info = paper.get(\"openAccessPdf\")\n",
        "    pdf_url = pdf_info.get(\"url\") if pdf_info else None\n",
        "    if not pdf_url:\n",
        "        return f\"❌ No PDF available for: {paper.get('title')}\"\n",
        "    try:\n",
        "        response = requests.get(pdf_url, stream=True, timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            filename = paper.get('title', 'paper').replace(\"/\", \"_\").replace(\" \", \"_\")[:50] + \".pdf\"\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            with open(filepath, \"wb\") as f:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "            return f\"✅ Downloaded: {filename}\"\n",
        "        else:\n",
        "            return f\"⚠️ Could not download PDF for: {paper.get('title')}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error downloading: {e}\"\n",
        "\n",
        "def literature_interface(query):\n",
        "    if not query.strip():\n",
        "        return \"Error: Please enter a search query.\"\n",
        "    history = load_history()\n",
        "    results = search_papers(query, limit=10)\n",
        "    ranked_results = rank_with_history(results, history)\n",
        "    if not ranked_results:\n",
        "        return \"No results found. Try different keywords.\"\n",
        "\n",
        "    display_text = \"\"\n",
        "    for i, paper in enumerate(ranked_results, 1):\n",
        "        authors = \", \".join([a.get('name', '') for a in paper.get('authors', [])])\n",
        "        title = paper.get('title', 'No title')\n",
        "        year = paper.get('year', 'Unknown')\n",
        "        url = paper.get('url', '')\n",
        "        abstract = str(paper.get('abstract', 'No abstract available.'))[:300] + \"...\"\n",
        "        display_text += f\"{i}. {title} ({year})\\nAuthors: {authors}\\nURL: {url}\\nAbstract: {abstract}\\n---\\n\"\n",
        "\n",
        "    if query:\n",
        "        history.append(query)\n",
        "        save_history(history)\n",
        "    return display_text\n",
        "# -------------------------------\n",
        "# Research Paper Field Extractor\n",
        "# -------------------------------\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"google/flan-t5-base\"\n",
        ")\n",
        "\n",
        "def extract_fields(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    text = text[:2000]\n",
        "    prompt = f\"\"\"\n",
        "    Extract the following fields from this research paper and format them clearly:\n",
        "    - Title\n",
        "    - Abstract summary\n",
        "    - Keywords\n",
        "    - Algorithms used\n",
        "    - Dataset names\n",
        "    - Results\n",
        "    - Limitations\n",
        "    - Conclusion\n",
        "    - Future enhancement\n",
        "    - DOI\n",
        "\n",
        "    Research Paper Text:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "    result = pipe(prompt, max_new_tokens=256)[0][\"generated_text\"]\n",
        "    return result.strip()\n",
        "\n",
        "# -------------------------------\n",
        "# Advanced Task Manager\n",
        "# -------------------------------\n",
        "class AdvancedTaskManager:\n",
        "    def __init__(self):\n",
        "        self.tasks = []\n",
        "        self.completed_tasks = []\n",
        "        self.task_history = []\n",
        "        self.vectorizer = TfidfVectorizer(max_features=100)\n",
        "        self.priority_classifier = MultinomialNB()\n",
        "        self.load_data()\n",
        "        self.train_priority_classifier()\n",
        "\n",
        "    def load_data(self):\n",
        "        if os.path.exists(\"tasks.json\"):\n",
        "            with open(\"tasks.json\", \"r\") as f:\n",
        "                self.tasks = [t for t in json.load(f) if t.get(\"status\") == \"Pending\"]\n",
        "        if os.path.exists(\"completed_tasks.json\"):\n",
        "            with open(\"completed_tasks.json\", \"r\") as f:\n",
        "                self.completed_tasks = [t for t in json.load(f) if t.get(\"status\") == \"Completed\"]\n",
        "        if os.path.exists(\"task_history.json\"):\n",
        "            with open(\"task_history.json\", \"r\") as f:\n",
        "                self.task_history = json.load(f)\n",
        "\n",
        "    def save_data(self):\n",
        "        with open(\"tasks.json\", \"w\") as f:\n",
        "            json.dump(self.tasks, f, indent=2)\n",
        "        with open(\"completed_tasks.json\", \"w\") as f:\n",
        "            json.dump(self.completed_tasks, f, indent=2)\n",
        "        with open(\"task_history.json\", \"w\") as f:\n",
        "            json.dump(self.task_history, f, indent=2)\n",
        "\n",
        "    def clear_all_tasks(self):\n",
        "        self.tasks, self.completed_tasks, self.task_history = [], [], []\n",
        "        self.save_data()\n",
        "        self.train_priority_classifier()\n",
        "        return \"All tasks and history cleared\", *self.get_task_lists()\n",
        "\n",
        "    def train_priority_classifier(self):\n",
        "        if self.task_history:\n",
        "            X = [t[\"task\"] for t in self.task_history]\n",
        "            y = [t[\"priority\"] for t in self.task_history]\n",
        "            if len(set(y)) > 1:\n",
        "                X_vec = self.vectorizer.fit_transform(X)\n",
        "                self.priority_classifier.fit(X_vec, y)\n",
        "\n",
        "    def suggest_priority(self, task):\n",
        "        if not task:\n",
        "            return \"Error: Please enter a task description\", \"\", \"\", \"\"\n",
        "        X_vec = self.vectorizer.transform([task])\n",
        "        if hasattr(self.priority_classifier, 'classes_'):\n",
        "            predicted_priority = self.priority_classifier.predict(X_vec)[0]\n",
        "        else:\n",
        "            urgent_keywords = [\"urgent\", \"critical\", \"deadline\", \"important\"]\n",
        "            predicted_priority = 1 if any(kw in task.lower() for kw in urgent_keywords) else 3\n",
        "        predicted_priority = max(1, min(5, int(predicted_priority)))\n",
        "        return f\"Suggested priority: {predicted_priority}\", \"\", \"\", \"\"\n",
        "\n",
        "    def suggest_deadline(self, task):\n",
        "        if not task:\n",
        "            return \"Error: Please enter a task description\", \"\", \"\", \"\"\n",
        "        workload = self.calculate_workload()\n",
        "        task_length = len(task.split())\n",
        "        suggested_days = 7 + (workload // 2)\n",
        "        if task_length > 20:\n",
        "            suggested_days += 2\n",
        "        if any(kw in task.lower() for kw in [\"urgent\", \"critical\", \"deadline\"]):\n",
        "            suggested_days = max(1, suggested_days - 2)\n",
        "        suggested_date = (datetime.now() + timedelta(days=suggested_days)).strftime(\"%Y-%m-%d\")\n",
        "        return f\"Suggested due date: {suggested_date}\", \"\", \"\", \"\"\n",
        "\n",
        "    def calculate_workload(self):\n",
        "        now = datetime.now()\n",
        "        two_weeks = now + timedelta(days=14)\n",
        "        return sum(1 for t in self.tasks if datetime.strptime(t[\"due_date\"], \"%Y-%m-%d\") <= two_weeks and t.get(\"status\") == \"Pending\")\n",
        "\n",
        "    def add_task(self, task, due_date, priority):\n",
        "        try:\n",
        "            if not due_date:\n",
        "                raise ValueError(\"Due date cannot be empty\")\n",
        "            datetime.strptime(due_date, \"%Y-%m-%d\")\n",
        "            if not priority:\n",
        "                _, _, _, priority = self.suggest_priority(task)\n",
        "            else:\n",
        "                priority = int(priority)\n",
        "                if not 1 <= priority <= 5:\n",
        "                    raise ValueError(f\"Priority must be 1-5 (got {priority})\")\n",
        "            if not task:\n",
        "                raise ValueError(\"Task description cannot be empty\")\n",
        "            new_task = {\"task\": task, \"due_date\": due_date, \"priority\": priority, \"status\": \"Pending\"}\n",
        "            self.tasks.append(new_task)\n",
        "            self.task_history.append({\"task\": task, \"due_date\": due_date, \"priority\": priority, \"completion_date\": None})\n",
        "            self.save_data()\n",
        "            self.train_priority_classifier()\n",
        "            return \"Task added successfully\", *self.get_task_lists()\n",
        "        except ValueError as e:\n",
        "            return f\"Error: Invalid input: {str(e)}\", *self.get_task_lists()\n",
        "\n",
        "    def mark_complete(self, task_index):\n",
        "        try:\n",
        "            task_index = int(task_index)\n",
        "            if task_index < 0 or task_index >= len(self.tasks):\n",
        "                return \"Error: Invalid task index\", \"\", \"\", \"\"\n",
        "            due_date = datetime.strptime(self.tasks[task_index][\"due_date\"], \"%Y-%m-%d\")\n",
        "            if due_date > datetime.now():\n",
        "                return \"Error: Cannot complete before due date\", \"\", \"\", \"\"\n",
        "            now_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "            task_to_complete = self.tasks.pop(task_index)\n",
        "            task_to_complete[\"status\"] = \"Completed\"\n",
        "            self.completed_tasks.append(task_to_complete)\n",
        "            for hist_task in self.task_history:\n",
        "                if hist_task[\"task\"] == task_to_complete[\"task\"] and hist_task[\"completion_date\"] is None:\n",
        "                    hist_task[\"completion_date\"] = now_date\n",
        "            self.save_data()\n",
        "            self.train_priority_classifier()\n",
        "            return \"Task marked as completed\", *self.get_task_lists()\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\", *self.get_task_lists()\n",
        "\n",
        "    def delete_task(self, task_index):\n",
        "        try:\n",
        "            task_index = int(task_index)\n",
        "            if task_index < 0 or task_index >= len(self.tasks):\n",
        "                return \"Error: Invalid task index\", \"\", \"\", \"\"\n",
        "            self.tasks.pop(task_index)\n",
        "            self.save_data()\n",
        "            return \"Task deleted successfully\", *self.get_task_lists()\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\", *self.get_task_lists()\n",
        "\n",
        "    def get_task_lists(self):\n",
        "        pending_tasks = \"\\n\".join([f\"{t['task']} | Due: {t['due_date']} | Priority: {t['priority']} | Status: {t['status']}\" for t in self.tasks if t.get(\"status\") == \"Pending\"])\n",
        "        completed_tasks = \"\\n\".join([f\"{t['task']} | Due: {t['due_date']} | Priority: {t['priority']} | Status: {t['status']}\" for t in self.completed_tasks if t.get(\"status\") == \"Completed\"])\n",
        "        total = len(self.tasks) + len(self.completed_tasks)\n",
        "        percentage = (len(self.completed_tasks) / total * 100) if total > 0 else 0\n",
        "        progress = f\"Progress: {percentage:.1f}% ({len(self.completed_tasks)}/{total} completed)\"\n",
        "        return pending_tasks, completed_tasks, progress\n",
        "\n",
        "# -------------------------------\n",
        "# Advanced Writing Support\n",
        "# -------------------------------\n",
        "class AdvancedWritingSupport:\n",
        "    def __init__(self):\n",
        "        self.grammar_pipeline = None\n",
        "        self.sbert_model = None\n",
        "\n",
        "    def load_models(self):\n",
        "        if self.grammar_pipeline is None:\n",
        "            self.grammar_pipeline = pipeline(\"text2text-generation\", model=\"vennify/t5-base-grammar-correction\")\n",
        "        if self.sbert_model is None:\n",
        "            self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def improve_writing(self, input_text):\n",
        "        self.load_models()\n",
        "        if not input_text.strip():\n",
        "            return \"Error: Enter input text\"\n",
        "        corrected = self.grammar_pipeline(f\"grammar: {input_text}\", max_length=200, truncation=True)[0]['generated_text']\n",
        "        return f\"Improved Text:\\n{corrected}\"\n",
        "\n",
        "    def check_plagiarism(self, input_text, source):\n",
        "        self.load_models()\n",
        "        if not input_text.strip() or not source.strip():\n",
        "            return \"Error: Enter text and source\"\n",
        "        if source.startswith(\"http\"):\n",
        "            try:\n",
        "                headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "                response = requests.get(source, headers=headers, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                source_text = soup.get_text(\" \", strip=True)\n",
        "            except Exception as e:\n",
        "                return f\"Error fetching URL: {str(e)}\"\n",
        "        else:\n",
        "            source_text = source\n",
        "        embed_input = self.sbert_model.encode(input_text)\n",
        "        embed_source = self.sbert_model.encode(source_text)\n",
        "        similarity = util.cos_sim(embed_input, embed_source)[0][0].item()\n",
        "        result = f\"Similarity Score: {similarity:.2f}\\n\"\n",
        "        if similarity > 0.8:\n",
        "            result += \"High similarity - Potential plagiarism\"\n",
        "        elif similarity > 0.5:\n",
        "            result += \"Moderate similarity - Review suggested\"\n",
        "        else:\n",
        "            result += \"Low similarity - Likely original\"\n",
        "        return result\n",
        "\n",
        "# -------------------------------\n",
        "# Initialize\n",
        "# -------------------------------\n",
        "task_manager = AdvancedTaskManager()\n",
        "writing_support = AdvancedWritingSupport()\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Unified Interface\n",
        "# -------------------------------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Unified Research, Literature Recommendation, Task & Writing Support Platform\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Literature Recommendation\"):\n",
        "            lit_query = gr.Textbox(label=\"Enter keywords\")\n",
        "            lit_button = gr.Button(\"Search Papers\")\n",
        "            lit_output = gr.Textbox(label=\"Search Results\", lines=20)\n",
        "            lit_button.click(fn=literature_interface, inputs=lit_query, outputs=lit_output)\n",
        "\n",
        "        with gr.TabItem(\"Research Paper Extractor\"):\n",
        "            pdf_input = gr.File(label=\"Upload Research Paper (PDF)\", type=\"filepath\")\n",
        "            pdf_output = gr.Textbox(label=\"Extracted Fields\", lines=15)\n",
        "            pdf_button = gr.Button(\"Extract\")\n",
        "            pdf_button.click(fn=extract_fields, inputs=pdf_input, outputs=pdf_output)\n",
        "\n",
        "        with gr.TabItem(\"Task Manager\"):\n",
        "            task_input = gr.Textbox(label=\"Task Description\")\n",
        "            due_date_input = gr.Textbox(label=\"Due Date (YYYY-MM-DD)\")\n",
        "            priority_input = gr.Textbox(label=\"Priority (1-5)\")\n",
        "            task_index = gr.Slider(label=\"Select Task Index\", minimum=0, maximum=100, step=1, value=0)\n",
        "            action = gr.Radio([\"Add Task\", \"Suggest Priority\", \"Suggest Deadline\", \"Mark Complete\", \"Delete Task\", \"Clear All Tasks\"], label=\"Action\")\n",
        "            task_button = gr.Button(\"Execute\")\n",
        "            task_output = gr.Textbox(label=\"Task Output\")\n",
        "            pending_tasks = gr.Textbox(label=\"Pending Tasks\", lines=10)\n",
        "            completed_tasks = gr.Textbox(label=\"Completed Tasks\", lines=10)\n",
        "            progress = gr.Textbox(label=\"Progress\")\n",
        "\n",
        "            def task_manager_interface(task, due_date, priority, task_index, action):\n",
        "                if action == \"Add Task\":\n",
        "                    return task_manager.add_task(task, due_date, priority)\n",
        "                elif action == \"Suggest Priority\":\n",
        "                    return task_manager.suggest_priority(task)\n",
        "                elif action == \"Suggest Deadline\":\n",
        "                    return task_manager.suggest_deadline(task)\n",
        "                elif action == \"Mark Complete\":\n",
        "                    return task_manager.mark_complete(task_index)\n",
        "                elif action == \"Delete Task\":\n",
        "                    return task_manager.delete_task(task_index)\n",
        "                elif action == \"Clear All Tasks\":\n",
        "                    return task_manager.clear_all_tasks()\n",
        "                return \"\", \"\", \"\", \"\"\n",
        "\n",
        "            task_button.click(\n",
        "                fn=task_manager_interface,\n",
        "                inputs=[task_input, due_date_input, priority_input, task_index, action],\n",
        "                outputs=[task_output, pending_tasks, completed_tasks, progress]\n",
        "            )\n",
        "\n",
        "        with gr.TabItem(\"Writing Support\"):\n",
        "            ws_input = gr.Textbox(label=\"Enter text\", lines=10)\n",
        "            ws_source = gr.Textbox(label=\"Enter source text or URL (for plagiarism check)\", lines=5)\n",
        "            ws_action = gr.Radio([\"Improve Writing\", \"Check Plagiarism\"], label=\"Choose Action\")\n",
        "            ws_button = gr.Button(\"Execute\")\n",
        "            ws_output = gr.Textbox(label=\"Output\", lines=15)\n",
        "\n",
        "            def writing_interface(text, source, action):\n",
        "                if action == \"Improve Writing\":\n",
        "                    return writing_support.improve_writing(text)\n",
        "                elif action == \"Check Plagiarism\":\n",
        "                    return writing_support.check_plagiarism(text, source)\n",
        "                return \"Select an action.\"\n",
        "\n",
        "            ws_button.click(\n",
        "                fn=writing_interface,\n",
        "                inputs=[ws_input, ws_source, ws_action],\n",
        "                outputs=ws_output\n",
        "            )\n",
        "\n",
        "demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "ncNmxKMM1loP",
        "outputId": "2dbf45e1-412b-46e5-a214-e01bc9b3511a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://95c9843c21e923769f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://95c9843c21e923769f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}